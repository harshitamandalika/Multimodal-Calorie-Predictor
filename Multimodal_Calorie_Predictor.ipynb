{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing the required libraries**"
      ],
      "metadata": {
        "id": "SF-KcthIzh7P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHXEE7o6oqZD"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import timedelta\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split,  TensorDataset\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Data**"
      ],
      "metadata": {
        "id": "H3bZq-Pt63-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(a) Data Preprocessing**"
      ],
      "metadata": {
        "id": "o7rcvlrbzt9E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkNWZz1KFUT3"
      },
      "source": [
        "### **CGM Data Preprocessing**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z848DpCCFYO5"
      },
      "outputs": [],
      "source": [
        "data_train_cgm = pd.read_csv('cgm_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of Train data:\", data_train_cgm.shape)"
      ],
      "metadata": {
        "id": "1zeeAzMb9UqK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSN_IFXCFZLO"
      },
      "outputs": [],
      "source": [
        "data_train_cgm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOKybeSJFaCP"
      },
      "outputs": [],
      "source": [
        "# Converting into DateTime objects\n",
        "data_train_cgm['Breakfast Time'] = pd.to_datetime(data_train_cgm['Breakfast Time'], errors = 'coerce')\n",
        "data_train_cgm['Lunch Time'] = pd.to_datetime(data_train_cgm['Lunch Time'], errors = 'coerce')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Handling Missing Values**"
      ],
      "metadata": {
        "id": "NTmB0PDB1KFQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT-wODaPFbW3"
      },
      "outputs": [],
      "source": [
        "data_train_cgm.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQcMDAFcFcnJ"
      },
      "outputs": [],
      "source": [
        "# Imputing null values using Forward Fill\n",
        "data_train_cgm['Breakfast Time'].fillna(method = 'ffill', inplace = True)\n",
        "data_train_cgm['Lunch Time'].fillna(method='ffill', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Xj2qBQCFdmh"
      },
      "outputs": [],
      "source": [
        "data_train_cgm['CGM Data'] = data_train_cgm['CGM Data'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "print(type(data_train_cgm['CGM Data'][0]))\n",
        "print(data_train_cgm['CGM Data'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7RxqkSXFffF"
      },
      "outputs": [],
      "source": [
        "data_cgm_num_tuples_before = data_train_cgm['CGM Data'].apply(len).sum()\n",
        "data_cgm_num_tuples_before"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Resampling and Interpolation of CGM Data**"
      ],
      "metadata": {
        "id": "UEOR2eU21lVV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqqoNG5PFgv7"
      },
      "outputs": [],
      "source": [
        "# Resampled to have entries every 5 minuites, corresponding glucose values are interpolated linearly\n",
        "subj_ids = data_train_cgm['Subject ID'].unique()\n",
        "day_nums = data_train_cgm['Day'].unique()\n",
        "\n",
        "data_train_cgm_resampled = pd.DataFrame()\n",
        "\n",
        "for subj_id in subj_ids:\n",
        "  for day_num in day_nums:\n",
        "    data_subj_id_day_num = data_train_cgm[(data_train_cgm['Subject ID'] == subj_id) & (data_train_cgm['Day'] == day_num)]\n",
        "\n",
        "    if data_subj_id_day_num.empty:\n",
        "      continue\n",
        "\n",
        "    data_all_cgm_row = []\n",
        "\n",
        "    for index, row in data_subj_id_day_num.iterrows():\n",
        "      data_train_cgm_row = row['CGM Data']\n",
        "\n",
        "      if not data_train_cgm_row or any(pd.isna(item[0]) for item in data_train_cgm_row):\n",
        "        continue\n",
        "\n",
        "      data_train_cgm_row_df = pd.DataFrame(data_train_cgm_row, columns=['timestamp', 'glucose_level'])\n",
        "\n",
        "      data_train_cgm_row_df['timestamp'] = pd.to_datetime(data_train_cgm_row_df['timestamp'], errors='coerce')\n",
        "\n",
        "      data_train_cgm_row_df = data_train_cgm_row_df.dropna(subset=['timestamp'])\n",
        "\n",
        "      if data_train_cgm_row_df.empty:\n",
        "        continue\n",
        "\n",
        "      start_time = data_train_cgm_row_df['timestamp'].min()\n",
        "      end_time = data_train_cgm_row_df['timestamp'].max()\n",
        "\n",
        "      if pd.isna(start_time) or pd.isna(end_time):\n",
        "        continue\n",
        "\n",
        "      resampled_time_index = pd.date_range(start=start_time, end=end_time, freq='5T')\n",
        "\n",
        "      if resampled_time_index.empty:\n",
        "        continue\n",
        "\n",
        "      data_train_cgm_row_df_resampled = data_train_cgm_row_df.set_index('timestamp').reindex(resampled_time_index)\n",
        "\n",
        "      data_train_cgm_row_df_resampled['glucose_level'] = data_train_cgm_row_df_resampled['glucose_level'].interpolate(method='linear')\n",
        "\n",
        "      data_resampled_cgm_row = list(zip(data_train_cgm_row_df_resampled.index, data_train_cgm_row_df_resampled['glucose_level']))\n",
        "\n",
        "      data_all_cgm_row.extend(data_resampled_cgm_row)\n",
        "\n",
        "      data_resampled_subj_day = pd.DataFrame({\n",
        "          'Subject ID': [subj_id],\n",
        "          'Day': [day_num],\n",
        "          'Breakfast Time': [data_subj_id_day_num['Breakfast Time'].iloc[0]],\n",
        "          'Lunch Time': [data_subj_id_day_num['Lunch Time'].iloc[0]],\n",
        "          'CGM Data': [data_all_cgm_row]})\n",
        "\n",
        "      data_train_cgm_resampled = pd.concat([data_train_cgm_resampled, data_resampled_subj_day], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSsbY2z6F9Y-"
      },
      "outputs": [],
      "source": [
        "data_train_cgm = data_train_cgm_resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um0mb7fGGM4b"
      },
      "outputs": [],
      "source": [
        "data_train_cgm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Visualizing the change in glucose level values with time**"
      ],
      "metadata": {
        "id": "99SVGiLBnxMz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVqxgyycGN2h"
      },
      "outputs": [],
      "source": [
        "data_subj_id_day_num = data_train_cgm[(data_train_cgm['Subject ID'] == 1) & (data_train_cgm['Day'] == 2)]\n",
        "\n",
        "data_cgm = data_subj_id_day_num['CGM Data'].iloc[0]\n",
        "\n",
        "timestamps, glucose_vals = zip(*data_cgm)\n",
        "\n",
        "timestamps = pd.to_datetime(timestamps, format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "time_breakfast = pd.to_datetime(data_subj_id_day_num['Breakfast Time'].iloc[0])\n",
        "time_lunch = pd.to_datetime(data_subj_id_day_num['Lunch Time'].iloc[0])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(timestamps, glucose_vals, linestyle='-', color='b', label='Glucose Level')\n",
        "\n",
        "plt.axvline(x = time_breakfast, color='green', linestyle=':', label='Breakfast Time')\n",
        "plt.axvline(x = time_lunch, color='red', linestyle=':', label='Lunch Time')\n",
        "\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Glucose Level')\n",
        "plt.title('CGM Data for Subject ID 1, Day 2')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYxXyevOGPXb"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df = pd.DataFrame(data_train_cgm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Applying Standardization to the Glucose values**"
      ],
      "metadata": {
        "id": "yizLa9lc2DOn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaqUAHgwGRgu"
      },
      "outputs": [],
      "source": [
        "all_glucose_vals = [glucose_val\n",
        "                      for data_cgm in data_train_cgm_df['CGM Data']\n",
        "                      for timestamp, glucose_val in data_cgm]\n",
        "\n",
        "data_mean_glucose_vals = np.mean(all_glucose_vals)\n",
        "data_std_dev_glucose_vals = np.std(all_glucose_vals)\n",
        "\n",
        "data_train_cgm_df['CGM Data'] = data_train_cgm_df['CGM Data'].apply(\n",
        "    lambda cgm_data: [(timestamp, (glucose_level - data_mean_glucose_vals) / data_std_dev_glucose_vals)\n",
        "                      for timestamp, glucose_level in cgm_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8IvDvjIGVN2"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmTFtKU4GXhw"
      },
      "outputs": [],
      "source": [
        "data_cgm_num_tuples_after = data_train_cgm_df['CGM Data'].apply(len).sum()\n",
        "data_cgm_num_tuples_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np1X0gZkGZQE"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df = pd.DataFrame(data_train_cgm_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfNoGZOyGaka"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Converting the Breakfast time, Lunch time and Timestamps into Seconds after midnight**"
      ],
      "metadata": {
        "id": "obSH3qIJ2VgM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txsuMSAkGbfA"
      },
      "outputs": [],
      "source": [
        "def time_to_seconds(time):\n",
        "  return int(timedelta(hours=time.hour, minutes=time.minute, seconds=time.second).total_seconds())\n",
        "\n",
        "data_train_cgm_df['Breakfast Time'] = pd.to_datetime(data_train_cgm_df['Breakfast Time'])\n",
        "data_train_cgm_df['Breakfast Time(Seconds after midnight)'] = data_train_cgm_df['Breakfast Time'].dt.time.apply(time_to_seconds)\n",
        "\n",
        "data_train_cgm_df['Lunch Time'] = pd.to_datetime(data_train_cgm_df['Lunch Time'])\n",
        "data_train_cgm_df['Lunch Time(Seconds after midnight)'] = data_train_cgm_df['Lunch Time'].dt.time.apply(time_to_seconds)\n",
        "\n",
        "def process_tuple_list(tuple_list):\n",
        "  return [(time_to_seconds(pd.to_datetime(t[0]).time()), *t[1:]) for t in tuple_list]\n",
        "\n",
        "data_train_cgm_df['CGM Data(seconds after midnight)'] = data_train_cgm_df['CGM Data'].apply(process_tuple_list)\n",
        "\n",
        "print(data_train_cgm_df[['Breakfast Time(Seconds after midnight)', 'Lunch Time(Seconds after midnight)', 'CGM Data(seconds after midnight)']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu7v_HnLHS7U"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRKE15KsHa_v"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TwzJH6uHf_F"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df = data_train_cgm_df.drop(['Breakfast Time', 'Lunch Time', 'CGM Data'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqtRXvHPHueT"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UPBHfw-Hv_l"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df.rename(columns={'Breakfast Time(Seconds after midnight)':'Breakfast Time', 'Lunch Time(Seconds after midnight)':'Lunch Time', 'CGM Data(seconds after midnight)': 'CGM Data'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GevnpnBQIPqP"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Waevh206IRLk"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df['CGM Data'] = data_train_cgm_df['CGM Data'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vlSwL0MIie1"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df['CGM Data'].apply(len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Applying Padding towards the end of CGM Data list (with zeros) to ensure uniform length for further processing**"
      ],
      "metadata": {
        "id": "XFH0iYh-2ppz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQs25O-JIj_s"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_max_len = (data_train_cgm_df['CGM Data'].apply(len)).max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LVbG0TdIl7C"
      },
      "outputs": [],
      "source": [
        "def data_train_cgm_list_pad(tuple_list, target_length, default_tuple=(0,0)):\n",
        "  return tuple_list + [default_tuple] * (target_length - len(tuple_list))\n",
        "\n",
        "data_train_cgm_df['CGM_Data_padded'] = data_train_cgm_df['CGM Data'].apply(lambda x: data_train_cgm_list_pad(x, data_train_cgm_max_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIYUbZXtIqUN"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df['CGM_Data_padded'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFLZaY56Isq8"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgQeJSfUIvf-"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df = data_train_cgm_df.drop('CGM Data', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqIQMnowI3xr"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df.rename(columns={'CGM_Data_padded': 'CGM Data'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoAs4k9gI5Ga"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACLfbdCPJXHR"
      },
      "outputs": [],
      "source": [
        "data_train_cgm_preprocessed = data_train_cgm_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DZgjGdQI9KH"
      },
      "source": [
        "### **Demoviome data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyVj6B4NI7Ar"
      },
      "outputs": [],
      "source": [
        "data_train_demo_viome = pd.read_csv(\"demo_viome_train.csv\")\n",
        "data_train_demo_viome.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_demo_viome_df = pd.DataFrame(data_train_demo_viome)\n",
        "print(\"Shape of Training DataFrame:\", data_train_demo_viome_df.shape)"
      ],
      "metadata": {
        "id": "lGHXK0G6uSQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking for Missing Values**"
      ],
      "metadata": {
        "id": "7656tvKu3UxW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaNX1LkDI_bh"
      },
      "outputs": [],
      "source": [
        "print(\"Number of Missing values in all features :\")\n",
        "data_train_demo_viome_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Performing One hot Encoding**"
      ],
      "metadata": {
        "id": "QPZ4jjW5u1Am"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P69v-pEJAuV"
      },
      "outputs": [],
      "source": [
        "data_train_demo_viome_df_encoded = pd.get_dummies(data_train_demo_viome_df, columns=['Race','Diabetes Status'], drop_first=False)\n",
        "data_train_demo_viome_df_encoded_filtered_columns = data_train_demo_viome_df_encoded.columns[data_train_demo_viome_df_encoded.columns.str.contains('Race|Diabetes Status')].tolist()\n",
        "\n",
        "print(\"The following columns have been encoded:\",data_train_demo_viome_df_encoded_filtered_columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_demo_viome_df_encoded[data_train_demo_viome_df_encoded_filtered_columns] = data_train_demo_viome_df_encoded[data_train_demo_viome_df_encoded_filtered_columns].astype(int)\n",
        "\n",
        "print(\"Data after One Hot Encoding:\")\n",
        "print(data_train_demo_viome_df_encoded)"
      ],
      "metadata": {
        "id": "xSiY1yjovpdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Separating all the PCA encoded Viome Data into different columns**"
      ],
      "metadata": {
        "id": "lfRd14o5vxCv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEl7VQ6ZJCdz"
      },
      "outputs": [],
      "source": [
        "data_train_demo_viome_df_split = data_train_demo_viome_df_encoded['Viome'].str.split(',', expand=True)\n",
        "data_train_demo_viome_df_split.columns = [f'Viome{i+1}' for i in range(data_train_demo_viome_df_split.shape[1])]\n",
        "data_train_demo_viome_df = data_train_demo_viome_df_encoded.drop('Viome', axis=1).join(data_train_demo_viome_df_split)\n",
        "\n",
        "print(\"Resulting DataFrame:\\n\", data_train_demo_viome_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Performing Standardization**"
      ],
      "metadata": {
        "id": "zPmz4nv8wVoi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnEkdPTCJEoI"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "columns_not_to_standardize = ('Subject ID', 'Gender') + tuple(data_train_demo_viome_df.columns[data_train_demo_viome_df.columns.str.contains('Race|Diabetes Status')].tolist())\n",
        "\n",
        "columns_to_standardize = [col for col in data_train_demo_viome_df.columns if col not in columns_not_to_standardize]\n",
        "print(\"The following columns are Standardized:\", columns_to_standardize)\n",
        "\n",
        "data_train_demo_viome_df_standardized = data_train_demo_viome_df.copy()\n",
        "data_train_demo_viome_df_standardized[columns_to_standardize] = scaler.fit_transform(data_train_demo_viome_df[columns_to_standardize])\n",
        "\n",
        "print(\"DemoViome Data after Standardization:\\n\", data_train_demo_viome_df_standardized)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Dropping unnecessary columns**"
      ],
      "metadata": {
        "id": "Jlkji95Jxw3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_demo_viome_df_standardized = data_train_demo_viome_df_standardized.drop(['Weight','Height','Race_African American',\n",
        "       'Race_Hispanic/Latino', 'Race_White','Viome11', 'Viome12', 'Viome13', 'Viome14', 'Viome15', 'Viome16',\n",
        "       'Viome17', 'Viome18', 'Viome19', 'Viome20', 'Viome21', 'Viome22',\n",
        "       'Viome23', 'Viome24', 'Viome25', 'Viome26', 'Viome27'], axis=1)"
      ],
      "metadata": {
        "id": "yZLBcqQiEUES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_demo_viome_df_standardized.columns"
      ],
      "metadata": {
        "id": "4v0NcSEQEnBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e5LpyVTJGli"
      },
      "outputs": [],
      "source": [
        "data_train_demo_viome_preprocessed = data_train_demo_viome_df_standardized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhdv131mJKhO"
      },
      "source": [
        "### **Image data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ-j4B3EJIwP"
      },
      "outputs": [],
      "source": [
        "data_train_image = pd.read_csv('img_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYMT0PXRJL_x"
      },
      "outputs": [],
      "source": [
        "data_train_image.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Resizing and Normalizing**"
      ],
      "metadata": {
        "id": "Bu4b_4_95FkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wzzuQuvJOKc"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "  image_array = np.array(eval(image_path), dtype=np.uint8)\n",
        "  image = Image.fromarray(image_array)\n",
        "  image = image.resize((128, 128))\n",
        "  image = np.array(image) / 255.0\n",
        "  return image\n",
        "\n",
        "data_train_image['Image Before Breakfast'] = data_train_image['Image Before Breakfast'].apply(preprocess_image)\n",
        "data_train_image['Image Before Lunch'] = data_train_image['Image Before Lunch'].apply(preprocess_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking for Missing values**"
      ],
      "metadata": {
        "id": "NHoea0Si5LZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjtZzbKpJPaS"
      },
      "outputs": [],
      "source": [
        "print(\"Number of Missing values in all features:\\n\")\n",
        "print(data_train_image.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-1_3niRJQy3"
      },
      "outputs": [],
      "source": [
        "data_train_image_preprocessed = data_train_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **(b) Data Preparation**"
      ],
      "metadata": {
        "id": "FF9QRgkP5ija"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OuDSAPDJmER"
      },
      "source": [
        "## **Merging data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yhZxQgPJjtq"
      },
      "outputs": [],
      "source": [
        "data_labels = pd.read_csv('label_train.csv')\n",
        "\n",
        "train_cgm = data_train_cgm_preprocessed\n",
        "train_demoviome = data_train_demo_viome_preprocessed\n",
        "train_image = data_train_image_preprocessed\n",
        "\n",
        "data_train_merged = pd.merge(data_labels, train_cgm, on=['Subject ID', 'Day'], how='inner')\n",
        "data_train_merged = pd.merge(data_train_merged, train_demoviome, on=['Subject ID'], how='inner')\n",
        "\n",
        "data_train_merged = pd.merge(data_train_merged, train_image, on=['Subject ID', 'Day'], how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klMR3DcbJqR-"
      },
      "outputs": [],
      "source": [
        "data_train_merged.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating a Multi Modal Dataset and saving to a DataLoader**"
      ],
      "metadata": {
        "id": "k1KCwHeG1QrT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHZ3JOZyJroR"
      },
      "outputs": [],
      "source": [
        "class MultiModalDataset(Dataset):\n",
        "  def __init__(self, train_data, train_data_cgm, train_data_demo_viome, train_data_image, train_data_meal_times, data_labels):\n",
        "    self.train_data = train_data\n",
        "    self.train_data_cgm = train_data_cgm\n",
        "    self.train_data_demo_viome = train_data_demo_viome\n",
        "    self.train_data_image = train_data_image\n",
        "    self.train_data_meal_times = train_data_meal_times\n",
        "    self.data_labels = data_labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.train_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    train_data_cgm_item = self.train_data[self.train_data_cgm].iloc[idx]\n",
        "    train_data_cgm_all = cgm_single_tensor(train_data_cgm_item)\n",
        "    train_cgm_tensor = train_data_cgm_all\n",
        "\n",
        "    train_demo_viome_tensor = torch.tensor(self.train_data[self.train_data_demo_viome].iloc[idx].values, dtype=torch.float32)\n",
        "\n",
        "    train_image1_tensor = self.image_processing(self.train_data[self.train_data_image[0]].iloc[idx])\n",
        "    train_image2_tensor = self.image_processing(self.train_data[self.train_data_image[1]].iloc[idx])\n",
        "\n",
        "    train_meal_times_tensor = torch.tensor(self.train_data[self.train_data_meal_times].iloc[idx].values, dtype=torch.float32)\n",
        "\n",
        "    data_labels_tensor = torch.tensor(self.train_data[self.data_labels].iloc[idx], dtype=torch.float32)\n",
        "\n",
        "    return train_cgm_tensor, train_demo_viome_tensor, train_image1_tensor, train_image2_tensor, train_meal_times_tensor,  data_labels_tensor\n",
        "\n",
        "  def image_processing(self, image):\n",
        "    image_tensor = torch.from_numpy(image).float()\n",
        "\n",
        "    if image_tensor.dim() == 2:\n",
        "      image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "    if image_tensor.shape[0] == 1:\n",
        "      image_tensor = image_tensor.repeat(3, 1, 1)\n",
        "\n",
        "    if image_tensor.shape[0] != 3 and image_tensor.shape[2] == 3:\n",
        "      image_tensor = image_tensor.permute(2, 0, 1)\n",
        "\n",
        "    return image_tensor\n",
        "\n",
        "def collate_fn(batch):\n",
        "  train_cgm_batch, train_demo_viome_batch, train_image1_batch, train_image2_batch,  train_meal_times_batch, data_labels_batch = zip(*batch)\n",
        "\n",
        "  train_cgm = torch.stack(train_cgm_batch)\n",
        "  train_demo_viome = torch.stack(train_demo_viome_batch)\n",
        "  train_image1 = torch.stack(train_image1_batch)\n",
        "  train_image2 = torch.stack(train_image2_batch)\n",
        "  train_meal_times = torch.stack(train_meal_times_batch)\n",
        "  data_labels = torch.stack(data_labels_batch)\n",
        "\n",
        "  return train_cgm, train_demo_viome, train_image1, train_image2, train_meal_times, data_labels\n",
        "\n",
        "def cgm_single_tensor(row):\n",
        "  cgm_flattened = [item for tuples in row for item in tuples]\n",
        "  return torch.tensor(cgm_flattened, dtype=torch.float32)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  train_data_cgm = 'CGM Data'\n",
        "  train_data_demo_viome = ['Age', 'Gender', 'A1C', 'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol', 'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio',\n",
        "                         'HOMA-IR', 'BMI', 'Diabetes Status_1', 'Diabetes Status_2', 'Diabetes Status_3', 'Viome1', 'Viome2', 'Viome3', 'Viome4', 'Viome5', 'Viome6', 'Viome7', 'Viome8', 'Viome9', 'Viome10']\n",
        "  train_data_image = ['Image Before Breakfast', 'Image Before Lunch']\n",
        "  train_data_meal_times = ['Breakfast Time', 'Lunch Time']\n",
        "  data_labels = ['Lunch Calories']\n",
        "\n",
        "  dataset = MultiModalDataset(data_train_merged, train_data_cgm, train_data_demo_viome, train_data_image, train_data_meal_times, data_labels)\n",
        "  dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "  for batch in dataloader:\n",
        "    cgm_train, demo_viome_train, image1_train, image2_train,  meal_times_train, labels = batch\n",
        "\n",
        "    print(\"Train CGM Data Shape:\", cgm_train.shape)\n",
        "    print(\"Train Demo_Viome Data Shape:\", demo_viome_train.shape)\n",
        "    print(\"Train Image 1 Data Shape:\", image1_train.shape)\n",
        "    print(\"Train Image 2 Data Shape:\", image2_train.shape)\n",
        "    print(\"Train Meal_Times Data Shape:\", meal_times_train.shape)\n",
        "    print(\"Data Labels Shape:\", labels.shape)\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLB1ZonPnFIn"
      },
      "source": [
        "## **Extracting individual modality data from dataloader**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CGM Data**"
      ],
      "metadata": {
        "id": "g10nR7Oo6SYP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlF3fnyAm1_Z"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "train_cgm_all = []\n",
        "for batch in dataloader:\n",
        "  cgm_train = batch[0]\n",
        "  train_cgm_all.append(cgm_train)\n",
        "\n",
        "full_train_data_cgm = torch.cat(train_cgm_all, dim=0)\n",
        "\n",
        "print(\"Shape of Complete CGM Data:\", full_train_data_cgm.shape)\n",
        "print(full_train_data_cgm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DemoViome Data**"
      ],
      "metadata": {
        "id": "xEMZeygB6Ze6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "train_demo_viome_all = []\n",
        "for batch in dataloader:\n",
        "  demo_viome_train = batch[1]\n",
        "  train_demo_viome_all.append(demo_viome_train)\n",
        "\n",
        "full_train_data_demo_viome = torch.cat(train_demo_viome_all, dim=0)\n",
        "\n",
        "print(\"Shape of Complete DemoViome Data:\", full_train_data_demo_viome.shape)\n",
        "print(full_train_data_demo_viome)"
      ],
      "metadata": {
        "id": "PSpLoKoIULRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Image 1**"
      ],
      "metadata": {
        "id": "Z6Pn1wyE6ga6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "train_image1_all = []\n",
        "for batch in dataloader:\n",
        "  image1_train = batch[2]\n",
        "  train_image1_all.append(image1_train)\n",
        "\n",
        "full_train_data_image1 = torch.cat(train_image1_all, dim=0)\n",
        "\n",
        "print(\"Shape of Complete Image1 Data:\", full_train_data_image1.shape)\n",
        "#print(full_train_data_image1)"
      ],
      "metadata": {
        "id": "QY6TyRhfULXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Image 2**"
      ],
      "metadata": {
        "id": "tzNt8yZ36qu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "train_image2_all = []\n",
        "for batch in dataloader:\n",
        "  image2_train = batch[3]\n",
        "  train_image2_all.append(image2_train)\n",
        "\n",
        "full_train_data_image2 = torch.cat(train_image2_all, dim=0)\n",
        "\n",
        "print(\"Shape of Complete Image2 Data:\", full_train_data_image2.shape)\n",
        "#print(full_train_data_image2)"
      ],
      "metadata": {
        "id": "o-fGBT6WULUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Meal Times**"
      ],
      "metadata": {
        "id": "j5eEuIcv6wx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "train_meal_times_all = []\n",
        "for batch in dataloader:\n",
        "  meal_times_train = batch[4]\n",
        "  train_meal_times_all.append(meal_times_train)\n",
        "\n",
        "full_train_meal_times = torch.cat(train_meal_times_all, dim=0)\n",
        "\n",
        "print(\"Shape of Complete Meal_Times Data:\", full_train_meal_times.shape)\n",
        "#print(full_train_meal_times)"
      ],
      "metadata": {
        "id": "qXonH60DXi4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **(c) Multimodal Model Implementation**"
      ],
      "metadata": {
        "id": "9UdcOFy77FX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encoder Transformer - CGM Data Embeddings**"
      ],
      "metadata": {
        "id": "I90Kpdhm7SWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderTransformer(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, num_heads, num_layers, ff_dim):\n",
        "    super(EncoderTransformer, self).__init__()\n",
        "    self.embedding = nn.Linear(input_dim, embedding_dim)\n",
        "    self.positional_encoding = nn.Parameter(torch.zeros(150, embedding_dim))\n",
        "    self.encoder_layers = nn.TransformerEncoder(\n",
        "        nn.TransformerEncoderLayer(\n",
        "            d_model = embedding_dim,\n",
        "            nhead = num_heads,\n",
        "            dim_feedforward = ff_dim,\n",
        "            dropout = 0.1),\n",
        "            num_layers = num_layers\n",
        "        )\n",
        "    self.pooling = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x) + self.positional_encoding\n",
        "    x = self.encoder_layers(x)\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = self.pooling(x).squeeze(-1)\n",
        "    return x\n",
        "\n",
        "full_train_data_cgm = full_train_data_cgm.view(full_train_data_cgm.size(0), 150, 2)\n",
        "\n",
        "Transformer_CGM_Model = EncoderTransformer(input_dim=2, embedding_dim=64, num_heads=4, num_layers=2, ff_dim=128)\n",
        "train_data_cgm_embeddings = Transformer_CGM_Model(full_train_data_cgm)\n",
        "print(\"Shape of Train Data CGM Embeddings:\", train_data_cgm_embeddings.shape)\n",
        "#print(train_data_cgm_embeddings)"
      ],
      "metadata": {
        "id": "ipNo-L5YQTuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fully Connected Neural Network - DemoViome Data Embeddings**"
      ],
      "metadata": {
        "id": "txkJ78bD7eu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FCNN(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size):\n",
        "    super(FCNN, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 64)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(64, embedding_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "FCNN_DemoViome_Model = FCNN(input_size=27, embedding_size=128)\n",
        "train_data_demo_viome_embeddings = FCNN_DemoViome_Model(full_train_data_demo_viome)\n",
        "\n",
        "print(\"Shape of Train Data DemoViome Embeddings:\", train_data_demo_viome_embeddings.shape)"
      ],
      "metadata": {
        "id": "S97xyeROIp4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Convolutional Neural Network - Image Data Embeddings**"
      ],
      "metadata": {
        "id": "tRsNYX1H7neb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNNModel, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(64 * 16 * 16, 512)\n",
        "    self.fc2 = nn.Linear(512, 128)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu(x)\n",
        "    return x\n",
        "\n",
        "CNN_Image_Model = CNNModel()\n",
        "\n",
        "train_data_image1_embeddings = CNN_Image_Model(full_train_data_image1)\n",
        "train_data_image2_embeddings = CNN_Image_Model(full_train_data_image2)\n",
        "\n",
        "print(\"Shape of Train Data Image 1 Embeddings:\", train_data_image1_embeddings.shape)\n",
        "print(\"Shape of Train Data Image 1 Embeddings:\", train_data_image2_embeddings.shape)\n",
        "#print(train_data_image1_embeddings)\n",
        "#print(train_data_image2_embeddings)"
      ],
      "metadata": {
        "id": "pBJNY9hrZvWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Combining Image Embeddings**"
      ],
      "metadata": {
        "id": "cM1R69Oz74SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concat_train_data_image_embeddings = torch.cat([train_data_image1_embeddings, train_data_image2_embeddings], dim=1)\n",
        "print(\"Concatenated Image Embeddings Shape:\", concat_train_data_image_embeddings.shape)"
      ],
      "metadata": {
        "id": "fz06M-OkQTox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We originally used all three embeddings, but image embeddings were not found useful, so later, we used only CGM and DemoViome embeddings"
      ],
      "metadata": {
        "id": "rhbCG-fX8DYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concat_train_data_embeddings = torch.cat([train_data_cgm_embeddings, train_data_demo_viome_embeddings], dim=1)\n",
        "print(\"Shape of Embeddings(2 Modalities):\", concat_train_data_embeddings.shape)"
      ],
      "metadata": {
        "id": "pgvUfR7jQTjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Joint Embedding**"
      ],
      "metadata": {
        "id": "v0oFGHYj8jtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final input to the dense layers\n",
        "full_train_data_embeddings = torch.cat([concat_train_data_embeddings, full_train_meal_times], dim=1)\n",
        "print(\"Shape of Entire Train Data Embeddings:\", full_train_data_embeddings.shape)"
      ],
      "metadata": {
        "id": "7Gr_eZC1QTg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Labels**"
      ],
      "metadata": {
        "id": "qbG29l1684YD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "data_labels_all = []\n",
        "for batch in dataloader:\n",
        "  label = batch[5]\n",
        "  data_labels_all.append(label)\n",
        "\n",
        "full_labels_data = torch.cat(data_labels_all, dim=0)\n",
        "\n",
        "print(\"Shape of Data Labels:\", full_labels_data.shape)\n",
        "#print(full_labels_data)"
      ],
      "metadata": {
        "id": "NMfw0R1wQTeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Data**"
      ],
      "metadata": {
        "id": "LsDYRi-xeOKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(a) Data preprocessing**"
      ],
      "metadata": {
        "id": "XMA_iTtR9GQ4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgk8cMuDWoHj"
      },
      "source": [
        "## **CGM Data Preprocessing**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHzSIYkBWoHl"
      },
      "outputs": [],
      "source": [
        "data_test_cgm = pd.read_csv('cgm_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of Test Data:\", data_test_cgm.shape)"
      ],
      "metadata": {
        "id": "sMwTehYsdP5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VgePWy2WoHl"
      },
      "outputs": [],
      "source": [
        "data_test_cgm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqC4b9vtWoHm"
      },
      "outputs": [],
      "source": [
        "# Converting into DateTime Objects\n",
        "data_test_cgm['Breakfast Time'] = pd.to_datetime(data_test_cgm['Breakfast Time'], errors = 'coerce')\n",
        "data_test_cgm['Lunch Time'] = pd.to_datetime(data_test_cgm['Lunch Time'], errors = 'coerce')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Handling Missing Values**"
      ],
      "metadata": {
        "id": "z3cgbRua-eQN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uFstUO2WoHm"
      },
      "outputs": [],
      "source": [
        "data_test_cgm.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvVrSwIiWoHn"
      },
      "outputs": [],
      "source": [
        "# Imputing Null Values using Forward Fill\n",
        "data_test_cgm['Breakfast Time'].fillna(method = 'ffill', inplace = True)\n",
        "data_test_cgm['Lunch Time'].fillna(method='ffill', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxtaggGGWoHn"
      },
      "outputs": [],
      "source": [
        "data_test_cgm['CGM Data'] = data_test_cgm['CGM Data'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "print(type(data_test_cgm['CGM Data'][0]))\n",
        "print(data_test_cgm['CGM Data'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne6raSPgWoHn"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_num_tuples_before = data_test_cgm['CGM Data'].apply(len).sum()\n",
        "data_test_cgm_num_tuples_before"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_cgm"
      ],
      "metadata": {
        "id": "aQUPGo_pY7-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Resampling and Interpolation of CGM Data**"
      ],
      "metadata": {
        "id": "eXcYYJXx-uTD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK7FjZJ0WoHn"
      },
      "outputs": [],
      "source": [
        "# Resampled to have entries every 5 minutes, corresponding glucose values are interpolated linearly\n",
        "subj_ids = data_test_cgm['Subject ID'].unique()\n",
        "day_nums = data_test_cgm['Day'].unique()\n",
        "\n",
        "data_test_cgm_resampled = pd.DataFrame()\n",
        "\n",
        "for subj_id in subj_ids:\n",
        "  for day_num in day_nums:\n",
        "    data_subj_id_day_num = data_test_cgm[(data_test_cgm['Subject ID'] == subj_id) & (data_test_cgm['Day'] == day_num)]\n",
        "\n",
        "    if data_subj_id_day_num.empty:\n",
        "      continue\n",
        "\n",
        "    data_all_cgm_row = []\n",
        "\n",
        "    for index, row in data_subj_id_day_num.iterrows():\n",
        "      data_test_cgm_row = row['CGM Data']\n",
        "\n",
        "      if not data_test_cgm_row or any(pd.isna(item[0]) for item in data_test_cgm_row):\n",
        "        continue\n",
        "\n",
        "      data_test_cgm_row_df = pd.DataFrame(data_test_cgm_row, columns=['timestamp', 'glucose_level'])\n",
        "\n",
        "      data_test_cgm_row_df['timestamp'] = pd.to_datetime(data_test_cgm_row_df['timestamp'], errors='coerce')\n",
        "\n",
        "      data_test_cgm_row_df = data_test_cgm_row_df.dropna(subset=['timestamp'])\n",
        "\n",
        "      if data_test_cgm_row_df.empty:\n",
        "        continue\n",
        "\n",
        "      start_time = data_test_cgm_row_df['timestamp'].min()\n",
        "      end_time = data_test_cgm_row_df['timestamp'].max()\n",
        "\n",
        "      if pd.isna(start_time) or pd.isna(end_time):\n",
        "        continue\n",
        "\n",
        "      resampled_time_index = pd.date_range(start=start_time, end=end_time, freq='5T')\n",
        "\n",
        "      if resampled_time_index.empty:\n",
        "        continue\n",
        "\n",
        "      data_test_cgm_row_df_resampled = data_test_cgm_row_df.set_index('timestamp').reindex(resampled_time_index)\n",
        "\n",
        "      data_test_cgm_row_df_resampled['glucose_level'] = data_test_cgm_row_df_resampled['glucose_level'].interpolate(method='linear')\n",
        "\n",
        "      data_resampled_cgm_row = list(zip(data_test_cgm_row_df_resampled.index, data_test_cgm_row_df_resampled['glucose_level']))\n",
        "\n",
        "      data_all_cgm_row.extend(data_resampled_cgm_row)\n",
        "\n",
        "      data_resampled_subj_day = pd.DataFrame({\n",
        "            'Subject ID': [subj_id],\n",
        "            'Day': [day_num],\n",
        "            'Breakfast Time': [data_subj_id_day_num['Breakfast Time'].iloc[0]],\n",
        "            'Lunch Time': [data_subj_id_day_num['Lunch Time'].iloc[0]],\n",
        "            'CGM Data': [data_all_cgm_row] })\n",
        "\n",
        "      data_test_cgm_resampled = pd.concat([data_test_cgm_resampled, data_resampled_subj_day], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhOeOXCEWoHo"
      },
      "outputs": [],
      "source": [
        "data_test_cgm = data_test_cgm_resampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc0PMDuHWoHo"
      },
      "outputs": [],
      "source": [
        "data_test_cgm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TA1LgewWoHo"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df = pd.DataFrame(data_test_cgm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Applying Standardization to the Glucose Values**"
      ],
      "metadata": {
        "id": "JToFTnMI_Izj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5uXfWbrWoHp"
      },
      "outputs": [],
      "source": [
        "all_glucose_vals = [glucose_val\n",
        "                      for data_cgm in data_test_cgm_df['CGM Data']\n",
        "                      for timestamp, glucose_val in data_cgm]\n",
        "\n",
        "data_mean_glucose_vals = np.mean(all_glucose_vals)\n",
        "data_std_dev_glucose_vals = np.std(all_glucose_vals)\n",
        "\n",
        "data_test_cgm_df['CGM Data'] = data_test_cgm_df['CGM Data'].apply(\n",
        "    lambda cgm_data: [(timestamp, (glucose_level - data_mean_glucose_vals) / data_std_dev_glucose_vals)\n",
        "                      for timestamp, glucose_level in cgm_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWcfL_pMWoHp"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgQTVrRFWoHp"
      },
      "outputs": [],
      "source": [
        "data_cgm_num_tuples_test_after = data_test_cgm_df['CGM Data'].apply(len).sum()\n",
        "data_cgm_num_tuples_test_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIB4m0M1WoHp"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df =pd.DataFrame(data_test_cgm_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNJ1DE_PWoHp"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Converting the Breakfast time, Lunch time and Timestamps into Seconds after midnight**"
      ],
      "metadata": {
        "id": "H83GWVhM_V3j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0oaw_pzWoHp"
      },
      "outputs": [],
      "source": [
        "def time_to_seconds(time):\n",
        "  return int(timedelta(hours=time.hour, minutes=time.minute, seconds=time.second).total_seconds())\n",
        "\n",
        "data_test_cgm_df['Breakfast Time'] = pd.to_datetime(data_test_cgm_df['Breakfast Time'])\n",
        "data_test_cgm_df['Breakfast Time(Seconds after midnight)'] = data_test_cgm_df['Breakfast Time'].dt.time.apply(time_to_seconds)\n",
        "\n",
        "data_test_cgm_df['Lunch Time'] = pd.to_datetime(data_test_cgm_df['Lunch Time'])\n",
        "data_test_cgm_df['Lunch Time(Seconds after midnight)'] = data_test_cgm_df['Lunch Time'].dt.time.apply(time_to_seconds)\n",
        "\n",
        "def process_tuple_list(tuple_list):\n",
        "  return [(time_to_seconds(pd.to_datetime(t[0]).time()), *t[1:]) for t in tuple_list]\n",
        "\n",
        "data_test_cgm_df['CGM Data(seconds after midnight)'] = data_test_cgm_df['CGM Data'].apply(process_tuple_list)\n",
        "\n",
        "print(data_test_cgm_df[['Breakfast Time(Seconds after midnight)', 'Lunch Time(Seconds after midnight)', 'CGM Data(seconds after midnight)']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "undFiVDoWoHp"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7g-Rq0tIWoHq"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdmyIP5qWoHq"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df = data_test_cgm_df.drop(['Breakfast Time', 'Lunch Time', 'CGM Data'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1FfeCNzWoHq"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVscqExvWoHq"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df.rename(columns={'Breakfast Time(Seconds after midnight)':'Breakfast Time', 'Lunch Time(Seconds after midnight)':'Lunch Time', 'CGM Data(seconds after midnight)': 'CGM Data'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJDIX7P-WoHq"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHO-06ZIWoHq"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df['CGM Data'] = data_test_cgm_df['CGM Data'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwORts9_WoHr"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df['CGM Data'].apply(len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Applying Padding towards the end of CGM Data list (with zeros) to ensure uniform length for further processing**"
      ],
      "metadata": {
        "id": "CYPs38im_uu4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiZhN-7aWoHr"
      },
      "outputs": [],
      "source": [
        "#getting the corresponding max value of train_data, since both train and test should be identical in length\n",
        "data_train_cgm_max_len = (data_train_cgm_df['CGM Data'].apply(len)).max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ooEfQSwWoHr"
      },
      "outputs": [],
      "source": [
        "def data_test_cgm_list_pad(tuple_list, target_length, default_tuple=(0,0)):\n",
        "  return tuple_list + [default_tuple] * (target_length - len(tuple_list))\n",
        "\n",
        "data_test_cgm_df['CGM_Data_padded'] = data_test_cgm_df['CGM Data'].apply(lambda x: data_test_cgm_list_pad(x, data_train_cgm_max_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5Zg8HKhWoHr"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df['CGM_Data_padded'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbU-Muf6WoHr"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqqQL8xXWoHs"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df = data_test_cgm_df.drop('CGM Data', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJnC7Qa-WoHs"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df.rename(columns={'CGM_Data_padded': 'CGM Data'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVGwAF6zWoHs"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCRBd85MWoHs"
      },
      "outputs": [],
      "source": [
        "data_test_cgm_preprocessed = data_test_cgm_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBTmRVANWoHt"
      },
      "source": [
        "## **DemoViome data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpYhWwUeWoHt"
      },
      "outputs": [],
      "source": [
        "data_test_demo_viome = pd.read_csv(\"demo_viome_test.csv\")\n",
        "data_test_demo_viome.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_demo_viome_df = pd.DataFrame(data_test_demo_viome)\n",
        "print(\"Shape of Test Data:\", data_test_demo_viome_df.shape)"
      ],
      "metadata": {
        "id": "uPYKY_YYWoHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAUnJDHQWoHt"
      },
      "outputs": [],
      "source": [
        "print(\"Number of Missing values in all features :\")\n",
        "data_test_demo_viome_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Performing One hot Encoding**"
      ],
      "metadata": {
        "id": "dwiGMxkhWoHt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA11O_7sWoHt"
      },
      "outputs": [],
      "source": [
        "data_test_demo_viome_df_encoded = pd.get_dummies(data_test_demo_viome_df, columns=['Race','Diabetes Status'], drop_first=False)\n",
        "data_test_demo_viome_df_encoded_filtered_columns = data_test_demo_viome_df_encoded.columns[data_test_demo_viome_df_encoded.columns.str.contains('Race|Diabetes Status')].tolist()\n",
        "\n",
        "print(\"The following columns have been encoded:\",data_test_demo_viome_df_encoded_filtered_columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_demo_viome_df_encoded[data_test_demo_viome_df_encoded_filtered_columns] = data_test_demo_viome_df_encoded[data_test_demo_viome_df_encoded_filtered_columns].astype(int)\n",
        "\n",
        "print(\"Data after One Hot Encoding:\")\n",
        "print(data_test_demo_viome_df_encoded)"
      ],
      "metadata": {
        "id": "7M1Md5vBWoHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Separating all the PCA encoded Viome Data into different columns**"
      ],
      "metadata": {
        "id": "Sci41jvmWoHu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXO3qV8AWoHu"
      },
      "outputs": [],
      "source": [
        "data_test_demo_viome_df_split = data_test_demo_viome_df_encoded['Viome'].str.split(',', expand=True)\n",
        "data_test_demo_viome_df_split.columns = [f'Viome{i+1}' for i in range(data_test_demo_viome_df_split.shape[1])]\n",
        "data_test_demo_viome_df = data_test_demo_viome_df_encoded.drop('Viome', axis=1).join(data_test_demo_viome_df_split)\n",
        "\n",
        "print(\"Resulting DataFrame:\\n\", data_test_demo_viome_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Performing Standardization**"
      ],
      "metadata": {
        "id": "QR-hvNyDWoHu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUKzqXNsWoHu"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "columns_not_to_standardize = ('Subject ID', 'Gender') + tuple(data_test_demo_viome_df.columns[data_test_demo_viome_df.columns.str.contains('Race|Diabetes Status')].tolist())\n",
        "\n",
        "columns_to_standardize = [col for col in data_test_demo_viome_df.columns if col not in columns_not_to_standardize]\n",
        "print(\"The following columns are Standardized:\", columns_to_standardize)\n",
        "\n",
        "data_test_demo_viome_df_standardized = data_test_demo_viome_df.copy()\n",
        "data_test_demo_viome_df_standardized[columns_to_standardize] = scaler.fit_transform(data_test_demo_viome_df[columns_to_standardize])\n",
        "\n",
        "print(\"DemoViome Data after Standardization:\\n\", data_test_demo_viome_df_standardized)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Dropping unnecessary columns**"
      ],
      "metadata": {
        "id": "l7m7LknsWoHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_demo_viome_df_standardized = data_test_demo_viome_df_standardized.drop(['Weight','Height','Race_African American',\n",
        "       'Race_Hispanic/Latino', 'Race_White','Viome11', 'Viome12', 'Viome13', 'Viome14', 'Viome15', 'Viome16',\n",
        "       'Viome17', 'Viome18', 'Viome19', 'Viome20', 'Viome21', 'Viome22',\n",
        "       'Viome23', 'Viome24', 'Viome25', 'Viome26', 'Viome27'], axis=1)"
      ],
      "metadata": {
        "id": "-EFCRckOWoHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test_demo_viome_df_standardized.columns"
      ],
      "metadata": {
        "id": "B9iqYNBmWoHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRSYbfQGWoHu"
      },
      "outputs": [],
      "source": [
        "data_test_demo_viome_preprocessed = data_test_demo_viome_df_standardized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7NNM0XHWoHu"
      },
      "source": [
        "## **Image data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "745dREPPWoHu"
      },
      "outputs": [],
      "source": [
        "data_test_image=pd.read_csv('img_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUNWJQ83WoHu"
      },
      "outputs": [],
      "source": [
        "data_test_image.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Resizing and Normalizing**"
      ],
      "metadata": {
        "id": "HXQHtPZQBawf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_35MSkGWoHu"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_path):\n",
        "  image_array = np.array(eval(image_path), dtype=np.uint8)\n",
        "  image = Image.fromarray(image_array)\n",
        "  image = image.resize((128, 128))\n",
        "  image = np.array(image) / 255.0\n",
        "  return image\n",
        "\n",
        "data_test_image['Image Before Breakfast'] = data_test_image['Image Before Breakfast'].apply(preprocess_image)\n",
        "data_test_image['Image Before Lunch'] = data_test_image['Image Before Lunch'].apply(preprocess_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking for Missing Values**"
      ],
      "metadata": {
        "id": "-Ww0gMHcBjdf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvo0zpxvWoHv"
      },
      "outputs": [],
      "source": [
        "print(\"Number of Missing values in all features:\\n\")\n",
        "print(data_test_image.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH9oDqdwWoHv"
      },
      "outputs": [],
      "source": [
        "data_test_image_preprocessed = data_test_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(b) Data Preparation**"
      ],
      "metadata": {
        "id": "Rv-3mzYdBsv4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdfyGyrIWoHv"
      },
      "source": [
        "## **Merging data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjMf5sBJWoHv"
      },
      "outputs": [],
      "source": [
        "test_cgm = data_test_cgm_preprocessed\n",
        "test_demoviome = data_test_demo_viome_preprocessed\n",
        "test_image = data_test_image_preprocessed\n",
        "\n",
        "data_test_merged = pd.merge(test_cgm, test_demoviome, on=['Subject ID'], how='inner')\n",
        "\n",
        "data_test_merged = pd.merge(data_test_merged, test_image, on=['Subject ID', 'Day'], how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdc-aAQxWoHv"
      },
      "outputs": [],
      "source": [
        "data_test_merged.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating a Multi Modal Dataset and saving to a DataLoader**"
      ],
      "metadata": {
        "id": "iZ1wavV_WoHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHLUhMAGWoHv"
      },
      "outputs": [],
      "source": [
        "class MultiModalDataset(Dataset):\n",
        "  def __init__(self, test_data, test_data_cgm, test_data_demo_viome, test_data_image, test_data_meal_times):\n",
        "    self.test_data = test_data\n",
        "    self.test_data_cgm = test_data_cgm\n",
        "    self.test_data_demo_viome = test_data_demo_viome\n",
        "    self.test_data_image = test_data_image\n",
        "    self.test_data_meal_times = test_data_meal_times\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.test_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    test_data_cgm_item = self.test_data[self.test_data_cgm].iloc[idx]\n",
        "    test_data_cgm_all = cgm_single_tensor(test_data_cgm_item)\n",
        "    test_cgm_tensor = test_data_cgm_all\n",
        "\n",
        "    test_demo_viome_tensor = torch.tensor(self.test_data[self.test_data_demo_viome].iloc[idx].values, dtype=torch.float32)\n",
        "\n",
        "    test_image1_tensor = self.image_processing(self.test_data[self.test_data_image[0]].iloc[idx])\n",
        "    test_image2_tensor = self.image_processing(self.test_data[self.test_data_image[1]].iloc[idx])\n",
        "\n",
        "    test_meal_times_tensor = torch.tensor(self.test_data[self.test_data_meal_times].iloc[idx].values, dtype=torch.float32)\n",
        "\n",
        "    return test_cgm_tensor, test_demo_viome_tensor, test_image1_tensor, test_image2_tensor, test_meal_times_tensor\n",
        "\n",
        "  def image_processing(self, image):\n",
        "    image_tensor = torch.from_numpy(image).float()\n",
        "\n",
        "    if image_tensor.dim() == 2:\n",
        "      image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "    if image_tensor.shape[0] == 1:\n",
        "      image_tensor = image_tensor.repeat(3, 1, 1)\n",
        "\n",
        "    if image_tensor.shape[0] != 3 and image_tensor.shape[2] == 3:\n",
        "      image_tensor = image_tensor.permute(2, 0, 1)\n",
        "\n",
        "    return image_tensor\n",
        "\n",
        "def collate_fn(batch):\n",
        "  test_cgm_batch, test_demo_viome_batch, test_image1_batch, test_image2_batch,  test_meal_times_batch = zip(*batch)\n",
        "\n",
        "  test_cgm = torch.stack(test_cgm_batch)\n",
        "  test_demo_viome = torch.stack(test_demo_viome_batch)\n",
        "  test_image1 = torch.stack(test_image1_batch)\n",
        "  test_image2 = torch.stack(test_image2_batch)\n",
        "  test_meal_times = torch.stack(test_meal_times_batch)\n",
        "\n",
        "  return test_cgm, test_demo_viome, test_image1, test_image2, test_meal_times\n",
        "\n",
        "def cgm_single_tensor(row):\n",
        "  cgm_flattened = [item for tuples in row for item in tuples]\n",
        "  return torch.tensor(cgm_flattened, dtype=torch.float32)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  test_data_cgm = 'CGM Data'\n",
        "  test_data_demo_viome = [ 'Age', 'Gender', 'A1C', 'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol', 'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio',\n",
        "                         'HOMA-IR', 'BMI', 'Diabetes Status_1', 'Diabetes Status_2', 'Diabetes Status_3', 'Viome1', 'Viome2', 'Viome3', 'Viome4', 'Viome5', 'Viome6', 'Viome7', 'Viome8', 'Viome9', 'Viome10']\n",
        "  test_data_image = ['Image Before Breakfast', 'Image Before Lunch']\n",
        "  test_data_meal_times = ['Breakfast Time', 'Lunch Time']\n",
        "\n",
        "  dataset = MultiModalDataset(data_test_merged, test_data_cgm, test_data_demo_viome, test_data_image, test_data_meal_times)\n",
        "  dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "  for batch in dataloader:\n",
        "    cgm_test, demo_viome_test, image1_test, image2_test,  meal_times_test = batch\n",
        "\n",
        "    print(\"Train CGM Data Shape:\", cgm_test.shape)\n",
        "    print(\"Train Demo_Viome Data Shape:\", demo_viome_test.shape)\n",
        "    print(\"Train Image 1 Data Shape:\", image1_test.shape)\n",
        "    print(\"Train Image 2 Data Shape:\", image2_test.shape)\n",
        "    print(\"Train Meal_Times Data Shape:\", meal_times_test.shape)\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Xg-mThWoHv"
      },
      "source": [
        "## **Extracting individual modality data fom dataloader**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CGM Data**"
      ],
      "metadata": {
        "id": "WYk-S4GsCKIG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzIb7BFjWoHv"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "test_cgm_all = []\n",
        "for batch in dataloader:\n",
        "  cgm_test = batch[0]\n",
        "  test_cgm_all.append(cgm_test)\n",
        "\n",
        "full_test_data_cgm = torch.cat(test_cgm_all, dim=0)\n",
        "\n",
        "print(\"Shape of Complete CGM Test Data:\", full_test_data_cgm.shape)\n",
        "print(full_test_data_cgm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DemoViome Data**"
      ],
      "metadata": {
        "id": "xjgsT2h3CPC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "test_demo_viome_all = []\n",
        "for batch in dataloader:\n",
        "  demo_viome_test = batch[1]\n",
        "  test_demo_viome_all.append(demo_viome_test)\n",
        "\n",
        "full_test_data_demo_viome = torch.cat(test_demo_viome_all, dim=0)\n",
        "\n",
        "print(\"Shape of Complete DemoViome Test Data:\", full_test_data_demo_viome.shape)\n",
        "print(full_test_data_demo_viome)"
      ],
      "metadata": {
        "id": "-yxozrZlWoHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Image 1**"
      ],
      "metadata": {
        "id": "q2RWWbxaCVPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "test_image1_all = []\n",
        "for batch in dataloader:\n",
        "  image1_test = batch[2]\n",
        "  test_image1_all.append(image1_test)\n",
        "\n",
        "full_test_data_image1 = torch.cat(test_image1_all, dim=0)\n",
        "\n",
        "print(\"Shape of Complete Image1 Test Data:\", full_test_data_image1.shape)\n",
        "#print(full_test_data_image1)"
      ],
      "metadata": {
        "id": "vPBwHFxLWoHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Image 2**"
      ],
      "metadata": {
        "id": "GjtKzP6cCZji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "test_image2_all = []\n",
        "for batch in dataloader:\n",
        "  image2_test = batch[3]\n",
        "  test_image2_all.append(image2_test)\n",
        "\n",
        "full_test_data_image2 = torch.cat(test_image2_all, dim=0)\n",
        "\n",
        "print(\"Shape of Complete Image2 Test Data:\", full_test_data_image2.shape)\n",
        "#print(full_test_data_image2)"
      ],
      "metadata": {
        "id": "u8rmYTQ_WoHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Meal Times**"
      ],
      "metadata": {
        "id": "uLMnoTcGCeLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "test_meal_times_all = []\n",
        "for batch in dataloader:\n",
        "  meal_times_test = batch[4]\n",
        "  test_meal_times_all.append(meal_times_test)\n",
        "\n",
        "full_test_meal_times = torch.cat(test_meal_times_all, dim=0)\n",
        "\n",
        "print(\"Shape of Complete Meal_Times Test Data:\", full_test_meal_times.shape)\n",
        "#print(full_test_meal_times)"
      ],
      "metadata": {
        "id": "ow8GajoTWoHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(c) Multimodal Model Implementation**"
      ],
      "metadata": {
        "id": "UtJZU5NaCkTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encoder Transformer - CGM Data Embeddings**"
      ],
      "metadata": {
        "id": "5GS8NPwpCsUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderTransformer(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, num_heads, num_layers, ff_dim):\n",
        "    super(EncoderTransformer, self).__init__()\n",
        "    self.embedding = nn.Linear(input_dim, embedding_dim)\n",
        "    self.positional_encoding = nn.Parameter(torch.zeros(150, embedding_dim))\n",
        "    self.encoder_layers = nn.TransformerEncoder(\n",
        "        nn.TransformerEncoderLayer(\n",
        "            d_model = embedding_dim,\n",
        "            nhead = num_heads,\n",
        "            dim_feedforward = ff_dim,\n",
        "            dropout = 0.1),\n",
        "            num_layers = num_layers\n",
        "        )\n",
        "    self.pooling = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x) + self.positional_encoding\n",
        "    x = self.encoder_layers(x)\n",
        "    x = x.permute(0, 2, 1)\n",
        "    x = self.pooling(x).squeeze(-1)\n",
        "    return x\n",
        "\n",
        "full_test_data_cgm = full_test_data_cgm.view(full_test_data_cgm.size(0), 150, 2)\n",
        "\n",
        "Transformer_CGM_Model = EncoderTransformer(input_dim=2, embedding_dim=64, num_heads=4, num_layers=2, ff_dim=128)\n",
        "test_data_cgm_embeddings = Transformer_CGM_Model(full_test_data_cgm)\n",
        "print(\"Shape of Train Data CGM Embeddings:\", test_data_cgm_embeddings.shape)\n",
        "#print(test_data_cgm_embeddings)"
      ],
      "metadata": {
        "id": "LahmI2JqWoHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fully Connected Neural Network - DemoViome Data Embeddings**"
      ],
      "metadata": {
        "id": "p-r41lVVC2la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FCNN(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size):\n",
        "    super(FCNN, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 64)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(64, embedding_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "FCNN_DemoViome_Model = FCNN(input_size=27, embedding_size=128)\n",
        "test_data_demo_viome_embeddings = FCNN_DemoViome_Model(full_test_data_demo_viome)\n",
        "\n",
        "print(\"Shape of Test Data DemoViome Embeddings:\", test_data_demo_viome_embeddings.shape)"
      ],
      "metadata": {
        "id": "1vn9G5BAWoHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Convolutional Neural Network - Image Data Embeddings**"
      ],
      "metadata": {
        "id": "t47zqmBVDC7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNNModel, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(64 * 16 * 16, 512)\n",
        "    self.fc2 = nn.Linear(512, 128)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu(x)\n",
        "    return x\n",
        "\n",
        "CNN_Image_Model = CNNModel()\n",
        "\n",
        "test_data_image1_embeddings = CNN_Image_Model(full_test_data_image1)\n",
        "test_data_image2_embeddings = CNN_Image_Model(full_test_data_image2)\n",
        "\n",
        "print(\"Shape of Test Data Image 1 Embeddings:\", test_data_image1_embeddings.shape)\n",
        "print(\"Shape of Test Data Image 1 Embeddings:\", test_data_image2_embeddings.shape)\n",
        "#print(test_data_image1_embeddings)\n",
        "#print(test_data_image2_embeddings)"
      ],
      "metadata": {
        "id": "2YGMlzZ3WoHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Combining Image Embeddings**"
      ],
      "metadata": {
        "id": "2HfztqtmDTsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concat_test_data_image_embeddings = torch.cat([test_data_image1_embeddings, test_data_image2_embeddings], dim=1)\n",
        "print(\"Concatenated Image Embeddings Shape:\", concat_test_data_image_embeddings.shape)"
      ],
      "metadata": {
        "id": "TlACkq44WoHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We originally used all three embeddings, but image embeddings were not useful, so later, we used only CGM and DemoViome embeddings"
      ],
      "metadata": {
        "id": "8FL72U-jDaze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concat_test_data_embeddings = torch.cat([test_data_cgm_embeddings, test_data_demo_viome_embeddings], dim=1)\n",
        "print(\"Shape of Embeddings(2 Modalities):\", concat_test_data_embeddings.shape)"
      ],
      "metadata": {
        "id": "dC_-W4o6WoHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Joint Embedding**"
      ],
      "metadata": {
        "id": "SUnDAhNWDw6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_test_data_embeddings = torch.cat([concat_test_data_embeddings, full_test_meal_times], dim=1)\n",
        "print(\"Shape of Entire Test Data Embeddings:\", full_test_data_embeddings.shape)"
      ],
      "metadata": {
        "id": "T4PFbChhWoHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(c) Dense Layers of the Multimodal Model, (d) Model training, and (e) Result analysis**"
      ],
      "metadata": {
        "id": "yHJ74V1pEDfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We performed hyperparameter tuning, and trained the model on the best set of hyperparameters and made predictions on test labels."
      ],
      "metadata": {
        "id": "8DsRvXhKGjiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The loss curve is also plotted below."
      ],
      "metadata": {
        "id": "UpsPRuCHG0A9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FinalDenseLayers(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super(FinalDenseLayers, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    self.fc3 = nn.Linear(128, output_size)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "class RMSRELoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(RMSRELoss, self).__init__()\n",
        "\n",
        "  def forward(self, y_pred, y_true):\n",
        "    relative_error = (y_pred - y_true) / (y_true)\n",
        "    squared_error = torch.square(relative_error)\n",
        "    mean_squared_error = torch.mean(squared_error)\n",
        "    rmsre = torch.sqrt(mean_squared_error)\n",
        "    return rmsre\n",
        "\n",
        "inputs = full_train_data_embeddings.detach()\n",
        "targets = full_labels_data.detach()\n",
        "dataset = TensorDataset(inputs, targets)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(full_test_data_embeddings, batch_size=32, shuffle=False)\n",
        "\n",
        "prediction_model = FinalDenseLayers(input_size=194, output_size=1)\n",
        "criterion = RMSRELoss()\n",
        "optimizer = torch.optim.Adam(prediction_model.parameters(), lr=0.005, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  prediction_model.train()\n",
        "  train_loss = 0.0\n",
        "  for inputs, targets in train_loader:\n",
        "    inputs, targets = inputs.to(torch.float32), targets.to(torch.float32)\n",
        "    optimizer.zero_grad()\n",
        "    predictions = prediction_model(inputs)\n",
        "    loss = criterion(predictions, targets)\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(prediction_model.parameters(), max_norm=0.5)\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  prediction_model.eval()\n",
        "  val_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for inputs, targets in val_loader:\n",
        "      inputs, targets = inputs.to(torch.float32), targets.to(torch.float32)\n",
        "      predictions = prediction_model(inputs)\n",
        "      loss = criterion(predictions, targets)\n",
        "      val_loss += loss.item()\n",
        "\n",
        "  train_loss /= len(train_loader)\n",
        "  val_loss /= len(val_loader)\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "  scheduler.step(val_loss)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, epochs + 1), train_losses, label=\"Training Loss\", color=\"blue\")\n",
        "plt.plot(range(1, epochs + 1), val_losses, label=\"Validation Loss\", color=\"red\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss (RMSRE)\")\n",
        "plt.title(\"Training and Validation Loss Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "itL26LAIWoHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Making Predictions on the Test data**"
      ],
      "metadata": {
        "id": "MS5dAmsjF6Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_model.eval()\n",
        "test_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_inputs in test_loader:\n",
        "    batch_inputs = batch_inputs.to(torch.float32)\n",
        "    predictions = prediction_model(batch_inputs)\n",
        "    test_predictions.append(predictions)\n",
        "\n",
        "test_predictions = torch.cat(test_predictions, dim=0).detach().numpy()\n",
        "\n",
        "row_id = np.arange(len(test_predictions))\n",
        "\n",
        "df_predictions = pd.DataFrame({\n",
        "    \"row_id\": row_id,\n",
        "    \"prediction\": test_predictions.flatten() })\n",
        "\n",
        "df_predictions.to_csv(\"test_predictions.csv\", index=False)\n",
        "#df_predictions"
      ],
      "metadata": {
        "id": "qne6anXiWnrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **We submitted these results to Kaggle and obtained an RMSRE Loss of 0.3374**"
      ],
      "metadata": {
        "id": "aEaZfowaGAC-"
      }
    }
  ]
}